import sys
import argparse
import collections
import logging
import pathlib
import pickle
from enum import Enum
# import enum
from pathlib import Path
from typing import Dict, List, Set, Sequence, Tuple, IO, Any
from datetime import datetime
import shutil

import os
import shlex
import subprocess
import re
import cmd2
import rich.console
import rich.filesize
import rich.table
import typer
from cmd2 import (Bg, Cmd2ArgumentParser, Fg, style, with_argparser)

import networkx as nx  # type: ignore
from networkx.drawing.nx_agraph import graphviz_layout

import bgraph
import bgraph.exc
import bgraph.utils

from bgraph.parsers.soong_parser import SoongFileParser, SoongParser
from bgraph.backup import (get_aosp_root, test_subdirs, backup_module, gen_backup_path, restore_module,
                           deep_dump_dict, is_empty_dir, is_normal_module_path, is_backup_module_path, conv_to_normal_path, write_line_to,
                           get_backup_root, gen_both_paths, get_module_dir, conv_to_backup_path, restore_from_backup_root, validate_module_path)
from bgraph.types import (Optional, OutChoice, QueryType, Section, SectionNode, Union,
                          is_defaults_module, get_backend_suffixed_names, setup_up_down_dependency, find_deep_dict,
                          check_host_supported_matching, check_if_cc_bin_unmatching, check_target_matching,
                          excluded_section_types, depends_on_impl, getSectionKey, has_invalid_target_arch,
                          check_section_name_matching, is_ndk_enabled_section, has_backend_dict, check_name_matching, excluded_sub_types)

import matplotlib.pyplot as plt
from natsort import natsort_key, natsort_keygen, ns

class DependType(Enum):
    UP = 1
    DOWN = 2
    BOTH = 3

logger: logging.Logger = bgraph.utils.create_logger(__name__)
"""Logger."""

def check_sections(up : SectionNode, down : SectionNode, dep_type : DependType) -> (bool, SectionNode, SectionNode):
    if dep_type is DependType.UP:
        return True, up, down
    elif dep_type is DependType.DOWN:
        return True, down, up
    else:
        return False, None, None


class SoongFile:

    def __init__(self, path : pathlib.Path = None):
        self.path : pathlib.Path = path
        self.section_nodes : Set[SectionNode] = set()

        self.up_soong_files : Set[SoongFile] = set()
        self.down_soong_files : Set[SoongFile] = set()
        self.child_soong_files : Set[SoongFile] = set()

    def find_section_node(self, name : str) -> Set[SectionNode]:
        result = set()
        for n in self.section_nodes:
            if n.section['section_name'] == name:
                result.add(n)
        return result

    def is_independent_module(self) -> bool:
        for uf in self.up_soong_files:
            if not self.path.parent in list(uf.path.parents):
                return False

        for cf in self.child_soong_files:
            for uf in cf.up_soong_files:
                if not self.path.parent in list(uf.path.parents):
                    return False
        return True

    def get_external_dep_files(self) -> Set[Any]:
        result : Set[SoongFile] = set()
        for uf in self.up_soong_files:
            if not self.path.parent in list(uf.path.parents):
                result.add(uf)

        for cf in self.child_soong_files:
            for uf in cf.up_soong_files:
                if not self.path.parent in list(uf.path.parents):
                    result.add(uf)

        return result

    def name(self) -> str:
        return str(self.path)

    def __key(self):
        return self.name()

    def __hash__(self):
        return hash(self.__key())

    def __eq__(self, other):
        if isinstance(other, SoongFile):
            return self.__key() == other.__key()
        else:
            raise LookupError

class Project:

    def __init__(self, path : pathlib.Path = None):
        self.path : pathlib.Path = path
        self.soong_files : Dict[pathlib.Path, List[SoongFile]] = collections.defaultdict(list)

        self.up_projects : Set[Project] = set()
        self.down_projects : Set[Project] = set()

    def hasSoongFile(self, file_path : pathlib.Path) -> bool:
        return bool(file_path in self.soong_files)

    def name(self) -> str:
        return str(self.path)

    def __key(self):
        return self.name()

    def __hash__(self):
        return hash(self.__key())

    def __eq__(self, other):
        if isinstance(other, Project):
            return self.__key() == other.__key()
        else:
            raise LookupError



def check_node_matching(up : SectionNode,
                        down : SectionNode) -> bool:
    if (up == down or
        up.section['section_type'] in excluded_section_types or
        down.section['section_type'] in excluded_section_types or
        up.section['section_name'] == down.section['section_name']):
        # logger.info("misc unmatch")
        return False
    elif is_defaults_module(up) or is_defaults_module(down):
        # logger.info("default module")
        return False
    elif check_target_matching(up, down) is False:
        # logger.info("target unmatch")
        return False
    elif check_if_cc_bin_unmatching(up, down) is True:
        # logger.info("cc bin unmatch")
        return False
    elif check_host_supported_matching(up, down) is False:
        # logger.info("host unmatch")
        return False
    else:
        return True


def load_modules_file(path : str) -> List[Path]:
    if not Path(path).exists():
        return []
    else:
        with open(path, 'r', encoding='utf-8') as file:
            return [ Path(line.strip()) for line in file.readlines() if line.strip() and not re.search(r"^#", line) ]


def check_if_in_modules(path : Path, module_paths : Set[Path]) -> bool:
    if not path.exists():
        return False
    elif path in module_paths:
        return True
    else:
        for m in module_paths:
            if m.exists():
                m_dir = m.parent if m.is_file() else m
                p_dir = path.parent if path.is_file() else path
                return m_dir == p_dir

        return False

def my_key_func(path : str|Path):
    if isinstance(path, str):
        path = Path(path)
    path = get_module_dir(path)
    natsort_get_key = natsort_keygen(alg=ns.NUMAFTER)
    return tuple(natsort_get_key(s) for s in path.parts)

def backup_file(path : Path|str):
    if isinstance(path, str):
        path = Path(path)
    if not path.exists():
        logger.error("non exist backup path:%s", str(path))
    else:
        date_time = datetime.now().strftime("%Y%d%m-%H%M%S")
        backup_path = path.with_name(f'{path.stem}-{date_time}{path.suffix}')
        path.rename(backup_path)
        logger.info("backup file from %s to %s", str(path), str(backup_path))
        write_line_to(path, 'w', f"# GEN with {backup_path.name}")

from filecmp import dircmp
def print_diff_files(dcmp, levels : int = 0):
    if dcmp.diff_files:
        logger.error("=== %d contains diff_files %d", levels, len(dcmp.diff_files))

    if dcmp.subdirs.values():
        logger.error("=== %d contains diff sub dirs %d", levels, len(dcmp.subdirs.values()))

    for name in dcmp.diff_files:
        logger.info("=== %d diff_file %s found in %s and %s", levels, name, dcmp.left, dcmp.right)
        
    for key, sub_dcmp in dcmp.subdirs.items():
        logger.info("== %d sub dirs:%s", levels, key)
        print_diff_files(sub_dcmp, levels + 1)

# dcmp = dircmp('dir1', 'dir2') 
# print_diff_files(dcmp)

def check_if_can_move(bp : Path) -> bool:
    res : bool = True
    same_file = 0
    same_dir = 0
    for f in os.listdir(str(bp)):
        dst = bp.parent / f
        if dst.exists():
            if dst.is_file():
                same_file += 1
                logger.info("same name file %s vs %s", str(dst), str(bp / f))
            else:
                same_dir += 1
                logger.info("same name DIR %s vs %s", str(dst), str(bp / f))
                src = bp / f
                dcmp = dircmp(str(src), str(dst))
                print_diff_files(dcmp)

            res = False

    logger.info("%s has same name DIR %d and file %d", str(bp), same_dir, same_file)

    return res


class GraphShell(cmd2.Cmd):

    pickle_file : Path = None

    CUSTOM_CATEGORY = 'My Custom Commands'

    def __init__(self):
        super().__init__(
            multiline_commands=['echo'],
            persistent_history_file='.gshell_history.dat',
            startup_script='scripts/startup.txt',
            include_ipy=True,
        )

        self.finished_set : Set[SectionNode] = set()
        self.query_dir_options = ['up', 'down', 'all']
        self.projects: Dict[pathlib.Path, Project] = collections.defaultdict(list)
        self.section_nodes : Set[SectionNode] = set()
        self.named_nodes_dict : Dict[str, Set[SectionNode]] = collections.defaultdict(set)
        self.all_module_types : Dict[str, int] = collections.defaultdict(int)

        # Prints an intro banner once upon application startup
        self.intro = style('Welcome to gshell!', fg=Fg.RED, bg=Bg.WHITE, bold=True)

        # Show this as the prompt when asking for input
        self.prompt = 'gshell> '

        # Used as prompt for multiline commands after the first line
        self.continuation_prompt = '... '

        # Allow access to your application in py and ipy via self
        self.self_in_py = True

        # Set the default category name
        self.default_category = 'cmd2 Built-in Commands'

        # Color to output text in with echo command
        self.foreground_color = Fg.CYAN.name.lower()

        # Make echo_fg settable at runtime
        fg_colors = [c.name.lower() for c in Fg]
        self.add_settable(
            cmd2.Settable('foreground_color', str, 'Foreground color to use with echo command', self, choices=fg_colors)
        )

    def project_provider(self) -> List[str]:
        """A choices provider is useful when the choice list is based on instance data of your application"""
        return [ str(project.path) for project in self.projects.values() ]

    def soong_file_provider(self, arg_tokens: Dict[str, List[str]]) -> List[str]:
        """
        a particular argument expects only 1 token.
        """
        if self.PROJECT in arg_tokens:
            project_path = pathlib.Path(arg_tokens[self.PROJECT][0])
            project = self.projects[project_path]
            return [ str(f.path) for f in project.soong_files.values() ]
        else:
            soong_file_paths = []
            for project in self.projects.values():
                for sf in project.soong_files.values():
                    soong_file_paths.append(str(sf.path))
            return soong_file_paths

    def section_provider(self, arg_tokens: Dict[str, List[str]]) -> List[str]:
        """
        a particular argument expects only 1 token.
        """
        if self.SOONG_FILE in arg_tokens:
            soong_path = pathlib.Path(arg_tokens[self.SOONG_FILE][0])
            for project in self.projects.values():
                for soong_file in project.soong_files.values():
                    if soong_file.path == soong_path:
                        return [ node.section['section_name'] for node in soong_file.section_nodes ]

            return []
        elif self.PROJECT in arg_tokens:
            project = self.projects[Path(arg_tokens[self.PROJECT][0])]
            if isinstance(project, Project):
                return [ node.section['section_name']
                         for soong_file in project.soong_files.values()
                         for node in soong_file.section_nodes ]
            else:
                return []
        else:
            return [ node.section['section_name'] for node in self.section_nodes ]


    @cmd2.with_category(CUSTOM_CATEGORY)
    def do_intro(self, _):
        """Display the intro banner"""
        self.poutput(self.intro)

    @cmd2.with_category(CUSTOM_CATEGORY)
    def do_echo(self, arg):
        """Example of a multiline command"""
        fg_color = Fg[self.foreground_color.upper()]
        self.poutput(style(arg, fg=fg_color))

    DEBUG_SECTION_NAMES = ("libminui", "vts_vndk_utils", "vndk_lib_lists")

    def setup_dependency_by_names(self,
                                  up : SectionNode,
                                  names : List[str],
                                  dep_way : str,
                                  edges : List[Tuple[SectionNode, SectionNode, Dict[str, any]]]):
        for name in names:
            if name and name[0] == ':':
                name = name [1:]
            down_nodes = self.named_nodes_dict.get(name, [])
            for down in down_nodes:
                if (dep_way == 'certificate' and
                    down.section['section_type'] != 'android_app_certificate'):
                    continue
                elif check_node_matching(up, down) or len(down_nodes) == 1:
                    setup_up_down_dependency(up, down, dep_way, edges)
                else:
                    pass
                        # logger.warning("node unmatching! up:%s down:%s", up.name2(), down.name2())

    def generate_edges_from_up(self,
                               up : SectionNode,
                               edges : List[Tuple[SectionNode, SectionNode, Dict[str, any]]]):
        for sub_type, names in up.section.items():
            if sub_type in excluded_sub_types:
                continue

            if isinstance(names, (str, list)):
                logger.info(">> sub_type:%s, names:%s", sub_type, repr(names))
                names = up.section[sub_type] if isinstance(up.section[sub_type], list) else [ up.section[sub_type] ]
                self.setup_dependency_by_names(up, names, sub_type, edges)
            elif isinstance(names, dict):
                result : Dict[Tuple, List[str]] = {}
                sub_types = ("binaries", "static_libs", "shared_libs", "header_libs",
                             "export_shared_lib_headers", "whole_static_libs", "srcs", "tools", "tool_files", "data", "pluginFor")
                for st in sub_types:
                    find_deep_dict(names, (sub_type, ), st, result)
                    for dep_way, names in result.items():
                        dep_way = '#'.join(dep_way)
                        self.setup_dependency_by_names(up, names, dep_way, edges)

        for (down, level) in up.defaults_nodes:
            if level == 0 and down not in up.down_level_nodes:
                setup_up_down_dependency(up, down, 'd_defaults', edges)


    def generate_edges_from_down(self,
                                 down_node : SectionNode,
                                 dep_type : DependType,
                                 edges : List[Tuple[SectionNode, SectionNode, Dict[str, any]]]):
        for up_node in self.section_nodes:
            success, up, down = check_sections(up_node, down_node, dep_type)
            if success:
                success, dep_way = depends_on_impl(up, down)
                if success:
                    edges.append((up, down, {'label' : dep_way}))
                    down.up_level_nodes.add(up)
                    down.up_level_dep_ways[up] = dep_way
                    up.down_level_nodes.add(down)
                    up.down_level_dep_ways[down] = dep_way

        up = down_node
        for (down, level) in up.defaults_nodes:
            if level == 0 and down not in up.down_level_nodes:
                setup_up_down_dependency(up, down, 'd_defaults', edges)


        self.finished_set.add(down_node)


    UNLIMITED_LEVELS : int = 1000000

    DEBUG_SECTION_NAMES = [ "liblpdump_interface", "lpdumpd" ]

    def setup_one_project_and_soong_deps(self,
                                         down_project : Project,
                                         down_soong_file : SoongFile,
                                         down_node : SectionNode):
        for up_node in down_node.up_level_nodes:
            up_project = self.projects[up_node.section['project_path']]
            if not isinstance(up_project, Project):
                raise LookupError

            down_project.up_projects.add(up_project)
            up_project.down_projects.add(down_project)

            up_soong_file = up_project.soong_files[up_node.section['soong_file_path']]
            if not isinstance(up_soong_file, SoongFile):
                raise LookupError

            down_soong_file.up_soong_files.add(up_soong_file)
            up_soong_file.down_soong_files.add(down_soong_file)


    def setup_project_and_soong_files_dep(self):
        for project in self.projects.values():
            for soong in project.soong_files.values():
                self.expand_soong_child(soong)
                for node in soong.section_nodes:
                    self.setup_one_project_and_soong_deps(project, soong, node)

    def build_node_graph(self,
                         max_levels : int,
                         node : SectionNode,
                         edges : List[Tuple[SectionNode, SectionNode, Dict[str, any]]]):
        if max_levels <= 0:
            logger.info("reached end of max_levels %d", max_levels)
        elif node in self.finished_set:
            logger.info("node %s already in finished set", node.name2())
        else:
            logger.info("with node:%s max_levels:%d", node.name2(), max_levels)
            # self.generate_edges_from_down(node, dep_type, edges)
            self.generate_edges_from_up(node, edges)

            # for up_node in node.up_level_nodes.difference(self.finished_set):
            #     self.build_node_graph(max_levels - 1, up_node, dep_type, edges)


    SECTION_NODES_FILE_PATH = "/home/richard/section-nodes.pkl"

    # defaults_set : Set[SectionNode]
    def find_expand_defaults_node(self,
                                  node : SectionNode,
                                  section_name : str,
                                  defaults_set : Set[SectionNode]) -> SectionNode:
        result : Set[SectionNode] = set()
        for n in defaults_set:
            if n.section['section_name'] == section_name:
                if n == node:
                    raise LookupError
                if not node.is_node_in_defaults(n):
                    result.add(n)
                else:
                    logger.info("already merged %s", section_name)
                    return None

        if len(result) == 1:
            return result.pop()
        elif len(result) > 1:
            for n in result:
                if node.section['soong_file_path'] == n.section['soong_file_path']:
                    logger.info("%d more than one, return the one in the same soong!", len(result))
                    return n
                elif node.section['project_path'] == n.section['project_path']:
                    logger.info("%d more than one, return one in the same project!", len(result))
                    return n

            logger.error("can only select first node!")
            return result.pop()
        else:
            logger.error("!!!! MISSED DEFAULTS NODE:%s for %s", section_name, node.name2())
            raise LookupError

    def dump_section(self, info : str, section : Section):
        if info == section['section_name']:
            logger.info("%s %s", info, section['section_name'])
            for key, value in section.items():
                logger.info("key:%s value:%s", key, repr(value))


    def merge_dict(self,
                   owner_dict : Dict,
                   slave_dict : Dict):
        for key, value in slave_dict.items():
            if key in ('section_name', 'section_type', 'project_path', 'soong_file_path', 'project_name'):
                pass
            elif key not in owner_dict:
                if isinstance(value, list):
                    owner_dict[key] = value.copy()
                else:
                    owner_dict[key] = value
                    # logger.info("merge node:%s new key:%s INTO owner", slave_dict['section_name'], key)
            elif not isinstance(owner_dict[key], type(value)):
                if isinstance(owner_dict[key], list):
                    owner_dict[key].append(value)
                elif isinstance(value, list):
                    owner_dict[key] = [ owner_dict[key], *value.copy() ]
                    # logger.info("after extend value of key %s, %s", key, repr(owner_dict[key]))
                else:
                    raise LookupError
            elif isinstance(value, list):
                ext_list = [ v for v in value if v not in owner_dict[key] ]
                if ext_list:
                    # logger.info("Extend [%s]: %s with %s", key, repr(owner_dict[key]), repr(ext_list))
                    owner_dict[key].extend(ext_list)
                else:
                    pass
                    # logger.info("empty ext_list! key:%s, value:[%s] org-value:[%s]", key, repr(value), repr(owner_dict[key]))
            elif isinstance(value, dict):
                self.merge_dict(owner_dict[key], value)
            elif owner_dict[key] != value:
                # logger.info("Overwrite [%s]: %s:%s to %s:%s", key, repr(owner_dict[key]), type(owner_dict[key]), repr(value), type(value))
                owner_dict[key] = value

    def merge_node(self,
                   level : int,
                   owner_node : SectionNode,
                   slave_node : SectionNode,
                   merged_nodes : Set[Tuple[SectionNode, int]]):
        self.merge_dict(owner_node.section, slave_node.section)
        merged_nodes.add((slave_node, level))

    def try_expand_defaults(self, node : SectionNode, defaults_set : Set[SectionNode]):
        level = 0
        while 'defaults' in node.section:
            defaults = node.section.pop('defaults')
            # logger.info("before expand:%s, DEFAULTS:%s", repr(node.section), repr(defaults))
            if isinstance(defaults, str):
                # logger.info("--> %s got a default name: %s", node.name2(), defaults)
                n = self.find_expand_defaults_node(node, defaults, defaults_set)
                if n:
                    # logger.info("<< find_expand_defaults_node %s [%s] got [%s]", node.name2(), defaults, n.name2())
                    self.merge_node(level, node, n, node.defaults_nodes)
            elif isinstance(defaults, list):
                for name in defaults:
                    # logger.info("%s got a defaults list: %s, name:%s", node.name2(), repr(defaults), name)
                    n = self.find_expand_defaults_node(node, name, defaults_set)
                    if n:
                        self.merge_node(level, node, n, node.defaults_nodes)
            else:
                raise LookupError

            if 'defaults' in node.section:
                defaults = node.section['defaults']
                logger.info("%s after DEFAULTS:%s", node.name2(), repr(defaults))

            level = level + 1

        # logger.info("After expand:%s", repr(node.section))

    def expand_default_sections(self, total_nodes : Set[SectionNode]):
        defaults_set = set()
        special_defaults = ("android_hardware_audio_config_default", "android_hardware_drm_1_0_multilib")
        defaults_types = [t for t in self.all_module_types if re.search(r"^\w+_defaults\Z", t) or t in special_defaults]
        logger.info("defaults_types:%s", repr(defaults_types))
        for node in self.section_nodes:
            if node.section['section_type'] in defaults_types:
                defaults_set.add(node)
                logger.info("got a defaults:%s", node.name2())

        # self.check_duplicated_name_sections(defaults_set)

        for n in total_nodes.difference(defaults_set):
            self.try_expand_defaults(n, defaults_set)

    def find_node_with_certificate(self) -> Set[SectionNode]:
        result = set()
        for node in self.section_nodes:
            if 'certificate' in node.section:
                result.add(node)
                logger.info("add node with certificate:%s", node.name2())
        return result

    def find_app_certificate_set(self) -> Set[SectionNode]:
        result = set()
        for node in self.section_nodes:
            if node.section['section_type'] == "android_app_certificate":
                result.add(node)
                logger.info("add a certificate:%s", node.name2())
        return result


    def expand_android_app_certificate_sections(self):
        certificate_node_set = self.find_app_certificate_set()
        for n in self.section_nodes.difference(certificate_node_set):
            self.try_expand_certificate(n, certificate_node_set)

    def try_expand_certificate(self,
                               owner_node : SectionNode,
                               certificate_node_set : Set[SectionNode]):
        while 'certificate' in owner_node.section:
            certificate = owner_node.section['certificate']
            del owner_node.section['certificate']

            # logger.info("before expand:%s, CERTIFICATE:%s", repr(owner_node.section), repr(certificate))
            if isinstance(certificate, str):
                logger.info("--> %s got a default name: %s", owner_node.name2(), certificate)
                n = self.find_expand_certificate_node(owner_node, certificate, certificate_node_set)
                if n:
                    # logger.info("<< find_expand_certificate_node %s [%s] got [%s]", owner_node.name2(), certificate, n.name2())
                    self.merge_certificate_node(owner_node, n)
            elif isinstance(certificate, list):
                for name in certificate:
                    # logger.info("%s got a certificate list: %s, name:%s", owner_node.name2(), repr(certificate), name)
                    n = self.find_expand_certificate_node(owner_node, name, certificate_node_set)
                    if n:
                        self.merge_certificate_node(owner_node, n)
            else:
                raise LookupError

            if 'certificate' in owner_node.section:
                certificate = owner_node.section['certificate']
                logger.info("%s after CERTIFICATE:%s", owner_node.name2(), repr(certificate))


        # logger.info("After expand:%s", repr(owner_node.section))

    def check_duplicated_name_sections(self, nodes : Set[SectionNode]):
        sections : Dict[str, Set[SectionNode]] = {}
        for node in nodes:
            name = node.section['section_name']
            if not sections.get(name):
                sections[name] = { node }
            elif not isinstance(sections[name], set):
                raise LookupError
            elif node in sections[name]:
                logger.error("node %s already in sections", node.name2())
                raise LookupError
            else:
                sections[name].add(node)

        uniq_cc = 0
        multi_cc = 0
        for name, nodes in sections.items():
            if len(nodes) ==  1:
                uniq_cc = uniq_cc + 1
            else:
                multi_cc = multi_cc + 1
                names = [ n.name2() for n in nodes ]
                logger.info("[%s]:", name)
                for s in names:
                    logger.info("\t[%s]", s)                    
                logger.info("\n")
                
        logger.info("got uniq:%d, multi:%d", uniq_cc, multi_cc)

        
    def do_check_duplicated_name_sections(self, _ : argparse.Namespace):
        self.check_duplicated_name_sections(self.section_nodes)


    def do_find_overlapped_certificate(self, _ : argparse.Namespace):
        certificate_node_set = self.find_app_certificate_set()
        i = 0
        total = 0
        for node in certificate_node_set:
            if 'certificate' in node.section:
                logger.info("%d:%d cert node %s contains another cert %s",
                            i, total, node.name2(), repr(node.section['certificate']))
                deep_dump_dict(node.section, 0, "dump app cert node")
                i = i + 1
            total = total + 1

    def do_find_node_with_certificate(self, _ : argparse.Namespace):
        nodes_with_cert = self.find_node_with_certificate()
        i = 0
        total = 0
        for node in nodes_with_cert:
            cert = node.section['certificate']
            if isinstance(cert, str):
                if re.search(r"^:", cert):
                    pass
                else:
                    logger.info(f"{i}/{total} node [%s] with cert:[%s]", node.name2(), node.section['certificate'])
                    i = i + 1
            else:
                raise LookupError
            total = total + 1

    def find_node_with_backend(self) -> Set[SectionNode]:
        result = set()
        for node in self.section_nodes:
            if has_backend_dict(node.section):
                result.add(node)
        return result

    def do_find_node_with_backend(self, _ : argparse.Namespace):
        nodes_with_backend = self.find_node_with_backend()
        i = 0
        for node in nodes_with_backend:
            logger.info(f"{i} node [%s] with backend:[%s]", node.name2(), node.section['backend'])
            deep_dump_dict(node.section['backend'], 1, "dump the backend")
            i = i + 1

    query_parser = Cmd2ArgumentParser(description="query section deps of a soong file of a project, with direct")

    @with_argparser(query_parser)
    def do_expand_sections(self, args : argparse.Namespace):
        total_nodes = self.section_nodes
        if args.section:
            total_nodes = self.find_section_node(args.project, args.soong_file, args.section)
        self.expand_default_sections(total_nodes)


    def expand_soong_child(self, sfile : SoongFile):
        root = Path(".")
        path = sfile.path.parent
        # logger.info("path:%s, root:%s", repr(path), repr(root))
        while path != root:
            # logger.info("-- path:%s, root:%s", repr(path), repr(root))
            soong_path = path.parent / 'Android.bp'
            if soong_path.exists():
                sf = self.find_soong_file(soong_path)
                if sf:
                    sf.child_soong_files.add(sfile)

            path = path.parent

    def expand_all_soong_up_files(self):
        for project in self.projects.values():
            for sf in project.soong_files.values():
                self.expand_soong_child(sf)


    @with_argparser(query_parser)
    def do_gen_nodes_graph(self, args : argparse.Namespace):
        self.finished_set.clear()

        total_nodes : Set[SectionNode] = self.section_nodes
        if args.project or args.soong_file or args.section:
            total_nodes = self.find_section_node(args.project, args.soong_file, args.section)

        for n in total_nodes:
            edges : List[Tuple[SectionNode, SectionNode, Dict[any]]] = []
            self.build_node_graph(self.UNLIMITED_LEVELS, n, edges)

        self.setup_project_and_soong_files_dep()

        # try:
        #     with open(self.SECTION_NODES_FILE_PATH, "wb") as file:
        #         pickle.dump(self.section_nodes, file)
        # except pickle.PickleError:
        #     logger.error("Failed to pickle dump section nodes to file %s", self.SECTION_NODES_FILE_PATH)

    def load_nodes_graph(self):
        with open(self.SECTION_NODES_FILE_PATH, "rb") as file:
            section_nodes = pickle.load(file)
            self.init_section_nodes(section_nodes)

    def do_load_nodes_graph(self, _ : argparse.Namespace):
        self.load_nodes_graph()

    dump_parser = Cmd2ArgumentParser(description="dump section nodes")
    dump_parser.add_argument('-l', '--max_levels', type=int, default=1, help='1 means only dump the leaf node')
    dump_parser.add_argument('-m', '--modules_file', type=str, default="modules.conf", help='dump android.bp path into module files')
    dump_parser.add_argument('-M', '--single_module', type=str, default=None, help='a module path or backup module path')
    dump_parser.add_argument('-s', '--success_modules_file', type=str, default="success-modules.conf", help='dump android.bp path into module files')
    dump_parser.add_argument('-f', '--failed_modules_file', type=str, default="failed-modules.conf", help='dump android.bp path into module files')
    dump_parser.add_argument('-F', '--fixed_modules_file', type=str, default="restore/fixed-modules.conf", help='dump android.bp path into module files')
    dump_parser.add_argument('-T', '--tools_modules_file', type=str, default="restore/tools-modules.conf", help='dump android.bp path into module files')
    dump_parser.add_argument('-a', '--all', action='store_true', help='dump all nodes')
    dump_parser.add_argument('-k', '--keep_going', action='store_true', default=False, help='dump all nodes')
    dump_parser.add_argument('-V', '--verbose', action='store_true', help='for validate soong file command')
    dump_parser.add_argument('-B', '--backup_modules_file', type=str, default="restore/backup-modules.conf", help='dump android.bp path into module files')
    dump_parser.add_argument('-d', '--dry_run', action='store_true', default=False, help='dry run')
    dump_parser.add_argument('-v', '--validate_paths', action='store_true', default=False, help='validate the backup/restore path list sequence')
    dump_parser.add_argument('--soong_file', choices_provider=soong_file_provider, metavar="SOONG", help="tab complete using a choices_provider")
    dump_parser.add_argument("--project", choices_provider=project_provider, metavar="PROJECT", help="tab complete using a choices_provider")

    def dump_types_provider(self) -> List[str]:
        return [ self.PROJECT, self.SOONG_FILE, self.SECTION, self.CHECK_CIRCULAR_DEP, self.ALL_MODULE_TYPES ]

    dump_parser.add_argument(
        "--type",
        choices_provider=dump_types_provider,
        metavar="TYPE",
        help="dump node types"
    )


    def dump_module_types_provider(self) -> List[str]:
        return list(self.all_module_types.keys())

    dump_parser.add_argument(
        "--module_type",
        choices_provider=dump_module_types_provider,
        metavar="MODTYPE",
        help="dump node with specified module type"
    )

    def dump_up_node_graph(self,
                           root : SectionNode,
                           levels : int,
                           max_levels : int) -> None:
        if levels < max_levels:
            if levels == 0:
                logger.info("root-up-node: %s", root.name2())

            tab = '--'
            levels = levels + 1
            for node in root.up_level_nodes:
                logger.info("%d %s: %s", levels, ''.join([c * levels for c in tab]), node.name2())
                self.dump_up_node_graph(node, levels, max_levels)

    def dump_down_node_graph(self,
                             root : SectionNode,
                             levels : int,
                             max_levels : int) -> None:
        if levels < max_levels:
            if levels == 0:
                logger.info("root-node: %s", root.name2())

            tab = '--'
            levels = levels + 1
            for node in root.down_level_nodes:
                logger.info("%d %s: %s", levels, ''.join([c * levels for c in tab]), node.name2())
                self.dump_down_node_graph(node, levels, max_levels)

    def dump_up_levels_node(self,
                            root : SectionNode,
                            levels : int,
                            max_up_levels : int) -> None:
        if levels < max_up_levels:
            tab = '^'
            levels = levels + 1
            for node in root.up_level_nodes:
                logger.info("%d %s: %s %s",
                            levels, ''.join([c * levels for c in tab]), node.name2(), root.up_level_dep_ways[node])
                self.dump_up_levels_node(node, levels, max_up_levels)

    def dump_down_levels_node(self,
                              root : SectionNode,
                              levels : int,
                              max_down_levels : int) -> None:
        if levels < max_down_levels:
            tab = 'v'
            levels = levels + 1
            for node in root.down_level_nodes:
                logger.info("%d %s: %s %s",
                            levels, ''.join([c * levels for c in tab]), node.name2(), root.down_level_dep_ways[node])
                self.dump_down_levels_node(node, levels, max_down_levels)


    def dump_both_levels_node(self,
                              root : SectionNode,
                              levels : int,
                              max_up_levels : int,
                              max_down_levels : int) -> None:
        logger.info("root-node: %s", root.name2())
        deep_dump_dict(root.section, 0, "root-node-dict")
        self.dump_up_levels_node(root, levels, max_up_levels)
        self.dump_down_levels_node(root, levels, max_down_levels)


    def check_if_circular_depended(self,
                                   node : SectionNode,
                                   down_node : SectionNode,
                                   checked_path : List[Tuple[SectionNode, str]],
                                   circular_nodes : Set[SectionNode],
                                   uncircular_nodes : Set[SectionNode]) -> bool:
        if node in circular_nodes:
            logger.info("=== Old Circular Node:%s", node.name2())
            return True
        elif node in uncircular_nodes:
            return False
        elif node in [ n for (n, _) in checked_path ]:
            logger.info("\n")
            for (n, dep_way) in checked_path:
                logger.info("In List:%s, dep:%s", n.name2(), dep_way)
            if down_node:
                logger.info("=== Target Circular Node:%s dep-way:%s", node.name2(), down_node.up_level_dep_ways[node])
            else:
                logger.info("=== Target Circular Node:%s dep-way:None", node.name2())
            circular_nodes.add(node)
            return True
        else:
            if not down_node:
                checked_path.append((node, "ROOT"))
            else:
                checked_path.append((node, down_node.up_level_dep_ways[node]))

            for n in node.up_level_nodes:
                if self.check_if_circular_depended(n, node, checked_path.copy(), circular_nodes, uncircular_nodes):
                    return True
                else:
                    uncircular_nodes.add(n)

            return False

    def check_if_any_circular_depended(self, nodes : Set[SectionNode]):
        circular_nodes = set()
        uncircular_nodes = set()
        for i, n in enumerate(nodes):
            if self.check_if_circular_depended(n, None, [], circular_nodes, uncircular_nodes):
                logger.info("%d got a circular dep node %s", i, n.name2())
            else:
                logger.info("%d got a normal dep node %s", i, n.name2())

        logger.info("there are %d circular dep nodes in these %d nodes", len(circular_nodes), len(nodes))

        for i, node in enumerate(circular_nodes):
            logger.info("%d circular node %s", i, node.name2())
            

    def dump_project_graph(self, project : Project, levels : int, max_levels : int) -> None:
        if levels < max_levels:
            if levels == 0:
                logger.info("root-project: %s", project.name())

            tab = '    '
            levels = levels + 1
            for down_pj in project.down_projects:
                logger.info("%d %s |---: %s", levels, ''.join([c * levels for c in tab]), down_pj.name())
                self.dump_project_graph(down_pj, levels, max_levels)

    def dump_project(self, project : Project) -> None:
        logger.info("project-name:%s", project.name())
        logger.info("project-path:%s", str(project.path))
        soong_file_paths = []
        for file in project.soong_files.values():
            soong_file_paths.append(str(file.path))
        logger.info("num-of-soong-files:%d", len(soong_file_paths))
        for i, sf in enumerate(soong_file_paths):
            logger.info("\t %d soong-file:%s", i, sf)


    def dump_soong_file(self, file : SoongFile, verbose : bool):
        ext_dep_files = file.get_external_dep_files()
        logger.info("dump soong file [%s], num of external deps:%d, num of nodes %d, num of up files %d, num of down files %d, num of children %d",
                    file.name(), len(ext_dep_files),
                    len(file.section_nodes), len(file.up_soong_files), len(file.down_soong_files), len(file.child_soong_files))

        logger.info("==== section nodes %d", len(file.section_nodes))
        for i, node in enumerate(file.section_nodes):
            logger.info("\t%d node %s, up nodes %d, down nodes %d",
                        i, node.name2(), len(node.up_level_nodes), len(node.down_level_nodes))

        if verbose is True:
            logger.info("---- down files %d", len(file.down_soong_files))
            for i, down_file in enumerate(file.down_soong_files):
                logger.info("\t%d down file %s, up files %d, down files %d",
                            i, down_file.name(), len(down_file.up_soong_files), len(down_file.down_soong_files))

        if verbose is True:
            logger.info("++++ up files %d", len(file.up_soong_files))
            for i, up_file in enumerate(file.up_soong_files):
                logger.info("\t%d up file %s, up files %d, down files %d",
                            i, up_file.name(), len(up_file.up_soong_files), len(up_file.down_soong_files))

        if verbose is True:
            logger.info("~~~~ child files %d", len(file.child_soong_files))
            for i, child in enumerate(file.child_soong_files):
                logger.info("\t%d child file %s, up files %d, down files %d",
                            i, child.name(), len(child.up_soong_files), len(child.down_soong_files))

        if verbose is True:
            logger.info("==== ext dep files %d", len(ext_dep_files))
            for i, ext_file in enumerate(ext_dep_files):
                logger.info("\t%d external file %s, up files %d, down files %d",
                            i, ext_file.name(), len(ext_file.up_soong_files), len(ext_file.down_soong_files))


    def dump_soong_file_with_level(self,
                                   levels : int,
                                   max_levels : int,
                                   undumped_files : Set[SoongFile],
                                   dumped_files : Set[SoongFile],
                                   modules_file : IO[Any]) -> None:
        if levels < max_levels:
            if not undumped_files:
                logger.info("REACHED THE END WITH FILE LEVELS %d", levels)
            else:
                tab = '--'
                once_dumped_files : Set[SoongFile] = set()
                for file in undumped_files:
                    logger.info("handle soong %s, up_soong_files:%d dumped_files:%d, undumped_files:%d",
                                file.path, len(file.up_soong_files), len(dumped_files), len(undumped_files))
                    valid_up_files = file.get_external_dep_files()
                    if (not valid_up_files or file.is_independent_module()):
                        once_dumped_files.add(file)
                        modules_file.write("%s\n" % str(file.path.absolute()))
                        # backup_module(file.path.absolute())
                        dumped_cc = len(dumped_files) + len(once_dumped_files)
                        logger.info("levels:%d dumped:%d %s:%s",
                                    levels, dumped_cc, ''.join([c * levels for c in tab]), file.name())
                        for i, node in enumerate(file.section_nodes):
                            logger.info("%d%s:%s", i, ''.join([c * (levels + 2) for c in tab]), node.name2())

                dumped_files.update(once_dumped_files)
                undumped_files = undumped_files.difference(dumped_files)
                self.dump_soong_file_with_level(levels + 1, max_levels, undumped_files, dumped_files, modules_file)
        else:
            logger.info("REACHED THE MAX FILE LEVEL %d %d", levels, max_levels)

    def dump_project_graph_with_level(self,
                                      levels : int,
                                      max_levels : int,
                                      undumped_projects : Set[Project],
                                      dumped_projects : Set[Project]) -> None:
        if levels < max_levels:
            if not undumped_projects:
                logger.info("REACHED THE END WITH PROJECT LEVELS %d", levels)
            else:
                tab = '--'
                once_dumped_projects : Set[Project] = set()
                for project in undumped_projects:
                    valid_up_projects = project.up_projects.difference(dumped_projects)
                    if not valid_up_projects:
                        once_dumped_projects.add(project)
                        dumped_cc = len(dumped_projects) + len(once_dumped_projects)
                        logger.info("levels:%d dumped:%d %s:%s",
                                    levels, dumped_cc, ''.join([c * levels for c in tab]), project.name())
                    for file in project.soong_files.values():
                        logger.info("%s:%s", ''.join([c * (levels + 2) for c in tab]), file.name())
                        for node in file.section_nodes:
                            logger.info("%s:%s", ''.join([c * (levels + 4) for c in tab]), node.name2())

                dumped_projects.update(once_dumped_projects)
                undumped_projects = undumped_projects.difference(dumped_projects)
                self.dump_project_graph_with_level(levels + 1, max_levels, undumped_projects, dumped_projects)
        else:
            logger.info("REACHED THE MAX PROJECT LEVEL %d %d", levels, max_levels)

    def dump_soong_file_graph(self, soong_file : SoongFile, levels : int, max_levels : int) -> None:
        if levels < max_levels:
            if levels == 0:
                logger.info("root-soong: %s", soong_file.name())
            tab = '    '
            levels = levels + 1
            for down_soong in soong_file.down_soong_files:
                logger.info("%d %s |---: %s", levels, ''.join([c * levels for c in tab]), down_soong.name())
                self.dump_soong_file_graph(down_soong, levels, max_levels)


    def dump_node_graph_with_level(self,
                                   levels : int,
                                   max_levels : int,
                                   undumped_nodes : Set[SectionNode],
                                   dumped_nodes : Set[SectionNode]) -> None:
        if levels < max_levels:
            if not undumped_nodes:
                logger.info("REACHED THE END WITH LEVELS %d", levels)
            else:
                tab = '--'
                once_dumped_nodes : Set[SectionNode] = set()
                for node in undumped_nodes:
                    valid_up_level_nodes = node.up_level_nodes.difference(dumped_nodes)
                    if not valid_up_level_nodes:
                        once_dumped_nodes.add(node)
                        dumped_cc = len(dumped_nodes) + len(once_dumped_nodes)
                        logger.info("levels:%d dumped:%d %s:%s", levels, dumped_cc, ''.join([c * levels for c in tab]), node.name2())
                dumped_nodes.update(once_dumped_nodes)
                undumped_nodes = undumped_nodes.difference(dumped_nodes)
                self.dump_node_graph_with_level(levels + 1, max_levels, undumped_nodes, dumped_nodes)
        else:
            logger.info("REACHED THE MAX LEVEL %d %d", levels, max_levels)

    def init_all_module_types(self, sections : Sequence[Section]):
        for section in sections:
            st = section['section_type']
            if st in self.all_module_types:
                self.all_module_types[st] = self.all_module_types[st] + 1
            else:
                self.all_module_types[st] = 1


    @with_argparser(dump_parser)
    def do_test_gen_path(self, args : argparse.Namespace):
        module_path, backup_path = gen_both_paths(Path(args.single_module))
        logger.info("%s --> module_path:%s backup_path:%s", args.single_module, module_path, backup_path)

    @with_argparser(dump_parser)
    def do_test_soong_file_parser(self, args : argparse.Namespace):
        parser = SoongFileParser(args.soong_file)


    @with_argparser(dump_parser)
    def do_dump_all_default_sections(self, args : argparse.Namespace):
        defaults_set : Set[SectionNode] = set()
        defaults_types = [t for t in self.all_module_types if re.search(r"^\w+_defaults\Z", t)]
        logger.info("defaults_types:%s", repr(defaults_types))
        for node in self.section_nodes:
            if node.section['section_type'] in defaults_types:
                defaults_set.add(node)
                logger.info("got a defaults:%s", node.name2())

        with open("./all-default-sections.txt", "w") as file:
            for i, node in enumerate(defaults_set):
                file.write('%-10d "%s"\n' % (i, node.section['section_name']))



    def dump_all_module_types(self):
        # module_types : Dict[str, int] = {}
        # for node in self.section_nodes:
        #     st = node.section['section_type']
        #     if st in module_types:
        #         module_types[st] = module_types[st] + 1
        #     else:
        #         module_types[st] = 1

        index = 0
        module_types = dict(sorted(self.all_module_types.items(), key=lambda x:x[1]))
        with open("./all-module-types.txt", "w") as file:
            file.write("[\n")
            for key, val in module_types.items():
                logger.info("%d type: [%s, %d]", index, key, val)
                index = index + 1
                file.write("\"%s\", %d\n" % (key, val))
            file.write("]\n")


    def diagnostic(self, soong_file_path : str, up_section_name : str, down_section_name : str):
        logger.error("MODULE:%s  UP-NODE:%s  DOWN-NODE:%s", soong_file_path, up_section_name, down_section_name)

        soong_file : SoongFile = self.find_soong_file(Path(soong_file_path))
        if not soong_file:
            logger.error("NO SOONG FILE:%s", soong_file_path)
            return

        up_nodes = soong_file.find_section_node(up_section_name)
        down_nodes = self.find_section_node(None, None, down_section_name)

        for i, up in enumerate(up_nodes):
            deep_dump_dict(up.section, 0, f"{i} up node:{up.name2()}")

        for i, down in enumerate(down_nodes):
            deep_dump_dict(down.section, 0, f"{i} down node:{down.name2()}")


    def compile_aosp(self,
                     module_path : Path = None,
                     fixed_modules_file : str = None) -> (bool, bool):
        command = shlex.split("bash -c 'source build/envsetup.sh && export ALLOW_MISSING_DEPENDENCIES=true && m'")
        logger.info("command:%s", repr(command))
        logger.info("module:[%s]", str(module_path))
        date_time = datetime.now().strftime("%Y%d%m-%H%M%S")
        with subprocess.Popen(command,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE) as proc:
            success : bool = False
            fixed_comment : str = None
            # device/google/crosshatch/Android.bp:1:1: module "soong_namespace": namespace hardware/google/interfaces does not exist
            # frameworks/rs/driver/runtime/rs_allocation.c:1:10: fatal error: 'rs_core.rsh' file not found
            # internal error: source path "frameworks/base/tools/hiddenapi/generate_hiddenapi_lists.py" does not exist
            # error: frameworks/base/Android.bp:477:1: module "framework-minus-apex" variant "android_common": module source path "frameworks/base/media/mca/filterpacks/java" does not exist
            # Import "frameworks/base/core/proto/android/stats/textclassifier/textclassifier_enums.proto" was not found or had errors.
            fixed_error_strings = ("missing and no known rule to make",
                                   "VNDK library list is not allowed",
                                   "module .* missing dependencies",
                                   r' module "\w+.*": namespace .* does not exist',
                                   r" fatal error: '\w+.*' file not found",
                                   r'source path ".*" does not exist',
                                   r'error: .* module "\w+.*" .* does not exist',
                                   r'error: symbol not found',
                                   r'error: could not resolve',
                                   r'Import ".*proto" was not found or had errors.',
                                   r'error: .* unrecognized module type "\w+.*"',
                                   )
            for line in proc.stdout:
                if isinstance(line, bytes):
                    line = line.decode('utf-8')
                line = line.strip()
                match = re.match(r'^error: (\w+.*Android.bp):\d+:\d+: "(\w+.*)" .* on undefined module "(\w+.*)"\Z', line)
                if match:
                    self.diagnostic(*match.groups())

                if module_path and fixed_modules_file:
                    for estr in fixed_error_strings:
                        if re.search(f'{estr}', line):
                            if fixed_comment is None:
                                fixed_comment = f'# GEN-{date_time}: {line}'
                            else:
                                fixed_comment += f'\n# {line}'
                if re.search(r'error|fail|unknown|ALLOW_MISSING_DEPENDENCIES|VNDK', line):
                    logger.error("%s", line)

                if line.find("soong bootstrap failed with:") != -1:
                    success = False
                elif line.find("build completed successfully") != -1:
                    success = True
            proc.communicate()

            if success is True:
                logger.info("== COMPILED MODULE %s ==", str(module_path))
            else:
                if module_path and fixed_modules_file:
                    if fixed_comment is None:
                        fixed_comment = "# Manually unknown error!"
                    write_line_to(fixed_modules_file, 'a', fixed_comment)
                    write_line_to(fixed_modules_file, 'a', str(module_path), True)

                with subprocess.Popen(f"bash ./show-compile-log.sh compilelog/compile{date_time}.log", stdout=subprocess.PIPE, shell=True) as proc:
                    proc.communicate()

            return success, fixed_comment is not None


    @with_argparser(dump_parser)
    def do_compile_aosp(self, args : argparse.Namespace):
        self.compile_aosp()

    @with_argparser(dump_parser)
    def do_update_fixed_modules(self, args : argparse.Namespace):
        fixed_modules_path = Path(args.fixed_modules_file)
        if not fixed_modules_path.is_file():
            raise LookupError

        logger.info("tools_modules_file %s", args.tools_modules_file)
        tools_modules = set(load_modules_file(args.tools_modules_file))
        for m in tools_modules:
            logger.info("tool module %s", m)

        date_time = datetime.now().strftime("%Y%d%m-%H%M%S")
        backup_fixed_modules_path = fixed_modules_path.with_name(f'{fixed_modules_path.stem}-{date_time}{fixed_modules_path.suffix}')
        fixed_modules_path.rename(backup_fixed_modules_path)
        logger.info("moved %s to %s", str(fixed_modules_path), str(backup_fixed_modules_path))
        write_line_to(args.fixed_modules_file, 'w', f"# GEN with {backup_fixed_modules_path.name}")

        modules_file : str = str(backup_fixed_modules_path)
        self.auto_backup(modules_file, args.fixed_modules_file, args.tools_modules_file,
                         args.success_modules_file, args.failed_modules_file, args.dry_run, args.keep_going)
        

    def auto_backup(self,
                    modules_file : str,
                    fixed_modules_file : str,
                    tools_modules_file : str,
                    success_modules_file : str,
                    failed_modules_file : str,
                    dry_run : bool,
                    keep_going : bool):
        fixed_modules = set(load_modules_file(fixed_modules_file))
        tools_modules = set(load_modules_file(tools_modules_file))
        with open(modules_file, 'r', encoding='utf-8') as file:
            paths = [Path(line.strip()) for line in file.readlines() if line.strip() and not re.search(r"^#", line)]
            if not paths:
                logger.error("no valid module path!")
            for path in paths:
                if not path.exists():
                    logger.info("non-exist-path: %s", path)
                elif path.suffix == '.bp' and path.name != 'Android.bp':
                    logger.error("ignore non Android.bp:%s", path)                    
                elif check_if_in_modules(path, fixed_modules):
                    logger.info("within fixed modules path: %s", path)
                elif check_if_in_modules(path, tools_modules):
                    logger.info("within tools modules path: %s", path)
                else:
                    backup_path = backup_module(path)
                    success, set_fixed = self.compile_aosp(path, fixed_modules_file)
                    if success:
                        logger.info("SUCCESS PATH:%s", path)
                        write_line_to(success_modules_file, 'a+', path, True)
                    else:
                        logger.info("FAILED PATH:%s", path)
                        write_line_to(failed_modules_file, 'a+', path, True)
                        restore_module(backup_path, dry_run)
                        if set_fixed is True:
                            fixed_modules.add(path)
                            logger.info("%s WAS SET FIXED, KEEP GOING", str(path))
                        elif keep_going is False:
                            logger.info("NO KEEP GOING, ENDED WITH MODULE:[%s]", path)
                            break

    @with_argparser(dump_parser)
    def do_auto_backup(self, args : argparse.Namespace):
        self.auto_backup(args.modules_file, args.fixed_modules_file, args.tools_modules_file,
                         args.success_modules_file, args.failed_modules_file, args.dry_run, args.keep_going)

    @with_argparser(dump_parser)
    def do_backup(self, args : argparse.Namespace):
        if args.single_module:
            path = Path(args.single_module)
            if path.exists():
                logger.info("backup single module path %s", path)
                backup_module(path)
            else:
                logger.error("non-exist single module path %s", path)
        else:
            module_paths = load_modules_file(args.modules_file)
            if module_paths:
                if args.validate_paths:
                    validate_module_path(module_paths, for_backup=True)
                for path in module_paths:
                    if path.exists():
                        logger.info(">> backup module %s", path)
                        backup_module(path, args.dry_run)
                    else:
                        logger.info("non-exist-mod-path %s!", path)
            else:
                logger.error("no valid module path!")


    @with_argparser(dump_parser)
    def do_restore(self, args : argparse.Namespace):
        if args.single_module:
            logger.info(">> restore with single module:%s", args.single_module)
            restore_module(Path(args.single_module), args.dry_run)
        else:
            module_paths = sorted(load_modules_file(args.modules_file), key=my_key_func, reverse=False)
            if module_paths:
                if args.validate_paths:
                    validate_module_path(module_paths, for_backup=False)
                for path in module_paths:
                    module_path, backup_path = gen_both_paths(path)
                    if backup_path.exists() and not module_path.exists():
                        logger.info(">> restore module %s", path)
                        restore_module(path, args.dry_run)
                    else:
                        logger.info("non-exist-mod-path %s!", path)
            else:
                logger.error("no valid module path!")


    def do_verify_bakcup_dir(self, args : argparse.Namespace):
        root = get_backup_root()
        for src in root.glob('*'):
            if src.is_file() and '#' in str(src):
                logger.info(">> gen_both_paths %s src", str(src))
                module_path, backup_path = gen_both_paths(src)
                logger.info("%s %s is a file", str(module_path), str(backup_path))

                if not module_path.exists():
                    module_path.mkdir(parents=True)
                    
                dst = module_path / 'Android.bp'
                if dst.is_file():
                    raise LookupError
                src.rename(dst)
                logger.info("### move %s to %s", str(src), str(dst))
                

    def do_clean_android_bp_dir_step5(self, args : argparse.Namespace):
        root = get_aosp_root()
        for rb in root.rglob('*richard-backup'):
            org = rb.parent / rb.name.split('-richard-backup')[0]

            if not org.is_dir():
                raise LookupError
            if not rb.is_dir():
                raise LookupError

            shutil.copytree(str(rb), str(org), dirs_exist_ok=True)
            logger.info("copytree src:%s dst:%s", str(rb), str(org))


    def do_clean_android_bp_dir_step4(self, args : argparse.Namespace):
        root = get_aosp_root()
        for bp in root.rglob('android-bp-temp-lly'):
            if not bp.is_dir():
                raise LookupError
            for f in os.listdir(str(bp)):
                src = bp / f
                dst = bp.parent / f
                if dst.exists():
                    dst.rename(dst.with_name(f'{dst.name}-richard-backup'))
                src.rename(dst)
                logger.info("mv %s to %s", str(src), str(dst))
            os.rmdir(str(bp))
            logger.info("---- removed dir %s\n", str(bp))


    def do_clean_android_bp_dir_step3(self, args : argparse.Namespace):
        root = get_aosp_root()
        for bp in root.rglob('android-bp-temp-lly'):
            if not bp.is_dir():
                raise LookupError
            if check_if_can_move(bp):
                for f in os.listdir(str(bp)):
                    src = bp / f
                    dst = bp.parent / f
                    src.rename(dst)
                    logger.info("mv %s to %s", str(src), str(dst))
                os.rmdir(str(bp))
                logger.info("---- removed dir %s\n", str(bp))

    def do_clean_android_bp_dir_step2(self, args : argparse.Namespace):
        root = get_aosp_root()
        total = 0;
        can_move = 0
        can_not_move = 0
        for bp in root.rglob('android-bp-temp-lly'):
            if not bp.is_dir():
                raise LookupError
            total += 1
            if check_if_can_move(bp):
                can_move += 1
                logger.info("%d ok, can move %s", can_move, str(bp))
            else:
                can_not_move += 1
                logger.info("%d ok, can not move %s", can_not_move, str(bp))

            logger.info("total:%d can_move:%d can_not_move:%d", total, can_move, can_not_move)

    def do_clean_android_bp_dir_step1(self, args : argparse.Namespace):
        root = get_aosp_root()
        for bp in root.rglob('Android.bp'):
            if bp.is_dir():
                tmp_bp = bp.with_name('android-bp-temp-lly')
                bp.rename(tmp_bp)
                logger.info("rename bp:%s to tmp_bp:%s", str(bp.relative_to(root)), str(tmp_bp.relative_to(root)))
                # for f in os.listdir(str(tmp_bp)):
                #     src = tmp_bp / f
                #     dst = src.parent.parent / src.name
                #     src.rename(dst)
                #     logger.info("mv %s to %s", str(src), str(dst))
                # os.rmdir(str(tmp_bp))
                # logger.info("---- removed dir %s\n", str(tmp_bp))


    def do_dir_android_bp(self, args : argparse.Namespace):
        root = get_aosp_root()
        dir_n = 0
        file_n = 0
        empty_n = 0
        non_bp_file = 0
        non_android_bp = 0
        non_empty_parent = 0
        for bp in root.rglob('Android.bp'):
            if bp.is_dir():
                dir_n += 1
                bp_file = bp / 'Android.bp'
                if not bp_file.exists():
                    non_bp_file += 1
                if bp.name != 'Android.bp':
                    non_android_bp += 1
                if len(os.listdir(str(bp.parent))) > 1:
                    non_empty_parent += 1
                    logger.info("NON-EMPTY-PARENT:%d path:%s", non_empty_parent, str(bp))

            else:
                file_n += 1

            if is_empty_dir(str(bp)):
                empty_n += 1

            logger.info("dir:%d file:%d empty:%d non-bp:%d non-and:%d non-empty-parent:%d path:%s",
                        dir_n, file_n, empty_n, non_bp_file, non_android_bp, non_empty_parent, str(bp))

    def do_test_natsort(self, args : argparse.Namespace):
        from natsort import natsort_keygen, ns
        # natsort_get_key = natsort_keygen(alg=ns.PATH|ns.PRESORT|ns.REAL)
        natsort_get_key = natsort_keygen(alg=ns.NUMAFTER)
        paths = (
            'test/vts-testcase/hal/wifi/V1_2/target_profiling/Android.bp',
            'test/vts-testcase/hal/wifi/V1_2/target_profiling/nan/Android.bp',
            'prebuilts/misc/common/robolectric/Android.bp',
            'prebuilts/misc/common/robolectric/4.3.1/Android.bp',
            'frameworks/rs/support.bp',
            'frameworks/rs/Android.bp',
            'frameworks/rs/support/jni/Android.bp'
            'frameworks/rs/support/jni/ncp/Android.bp'
        )
        for p in paths:
            tp = tuple(natsort_get_key(s) for s in Path(p).parts)
            logger.info("%s sorted as %s", p, repr(tp))

        logger.info("================")

        mpaths = [Path(p).absolute() for p in sorted(paths, key=my_key_func, reverse=True)]
        validate_module_path(mpaths, for_backup=True)

        for mp in mpaths:
            logger.info("mp:%s", repr(mp))



    def sort_modules_file(self, file_path : str):
        module_paths = sorted(load_modules_file(file_path), key=my_key_func, reverse=True)
        if module_paths:
            logger.info(">> validate_module_path %d", len(module_paths))
            validate_module_path(module_paths, for_backup=True)
            logger.info("<< validate_module_path %d", len(module_paths))
            with open(file_path, 'w', encoding='utf-8') as file:
                for path in module_paths:
                    file.write(f"{path}\n")
        else:
            logger.error("no module path load from %s", file_path)
                    

    @with_argparser(dump_parser)
    def do_restore_all_backup(self, args : argparse.Namespace):
        backup_file(args.backup_modules_file)
        with open(args.backup_modules_file, 'a', encoding='utf-8') as file:
            module_paths : List[Path] = restore_from_backup_root(args.dry_run)
            for path in module_paths:
                file.write("%s\n" % str(path))

    @with_argparser(dump_parser)
    def do_merge_modules_file(self, args : argparse.Namespace):
        files = ("backup-modules.conf", "backup-modules-20230408-125715.conf", "restore/backup-modules.conf", "success-modules.conf")
        modules = set()
        for f in files:
            modules.update(set(load_modules_file(f)))

        modules = sorted(modules, key=my_key_func, reverse=True)

        logger.info(">> validate_module_path %d", len(modules))
        validate_module_path(modules, for_backup=True)
        logger.info("<< validate_module_path %d", len(modules))
        with open("restore/merged-modules.conf", 'w', encoding='utf-8') as file:
            for path in modules:
                file.write(f"{path}\n")

    @with_argparser(dump_parser)
    def do_dump(self, args : argparse.Namespace):
        total_nodes = set()
        if not args.module_type:
            total_nodes = self.section_nodes
        else:
            for node in self.section_nodes:
                if node.section['section_type'] == args.module_type:
                    total_nodes.add(node)
                    logger.info("%d added node:[%s] with type:[%s] into total set", len(total_nodes), node.name2(), args.module_type)

        if args.type == self.ALL_MODULE_TYPES:
            self.dump_all_module_types()
        elif args.type == self.CHECK_CIRCULAR_DEP:
            self.check_if_any_circular_depended(total_nodes)
        elif args.type == self.SECTION:
            if args.all:
                self.dump_node_graph_with_level(0, args.max_levels, total_nodes, set())
            else:
                for node in total_nodes:
                    if not node.up_level_nodes: # start from leaf node
                        self.dump_down_node_graph(node, 0, args.max_levels)
        elif args.type == self.PROJECT:
            logger.info("dump project")
            if args.all:
                total_projects = set()
                for project in self.projects.values():
                    total_projects.add(project)
                logger.info("num of total projects:%d", len(total_projects))
                self.dump_project_graph_with_level(0, args.max_levels, total_projects, set())
            else:
                path : Path = Path(args.project)
                if path.is_dir():
                    project = self.projects[path]
                    if isinstance(project, Project):
                        self.dump_project(project)
                    else:
                        logger.error("invalid project path %s", str(path))
                    
        elif args.type == self.SOONG_FILE:
            logger.info("dump soong file")
            total_files = set()
            with open(args.modules_file, 'w', encoding='utf-8') as modules_file:
                path = Path(args.soong_file) if args.soong_file else None
                for project in self.projects.values():
                    for file in project.soong_files.values():
                        if path is None or file.path == path:
                            total_files.add(file)
                            logger.info("added file %s", file.path)

                if len(total_files) == 1:
                    self.dump_soong_file(total_files.pop(), args.verbose)
                else:
                    self.dump_soong_file_with_level(0, args.max_levels, total_files, set(), modules_file)
                    self.sort_modules_file(args.modules_file)

        else:
            raise LookupError("invalid args type")


    def gen_section_up_graph(self, down_node : SectionNode, max_levels : int):
        graph = nx.DiGraph()
        # graph = nx.complete_graph(3, create_using=nx.DiGraph)
        # graph = nx.balanced_tree(r=5, h=6, create_using=nx.DiGraph)

        self.finished_set.clear()
        edges : List[Tuple[SectionNode, SectionNode, Dict[str, any]]] = []

        self.build_node_graph(max_levels, down_node, edges)

        graph.add_node(down_node, label=down_node.name())

        for up_node, down_node, label in edges:
            graph.add_node(up_node, label=up_node.name())
            # graph.add_node(up_node, label=getSectionKey(up_node.section))
            # graph.add_node(down_node, label=getSectionKey(down_node.section))

        graph.add_edges_from(edges)


        logger.info(">> graphviz_layout")
        # pos = graphviz_layout(graph, prog="twopi")
        pos = graphviz_layout(graph, prog="neato")
        # pos = graphviz_layout(graph, prog="dot")
        # pos = graphviz_layout(graph, prog="circo")
        logger.info(">> draw")
        # nx.draw_networkx_edge_labels(graph, pos)

        nx.draw_networkx_nodes(graph, pos, node_size=50, node_color='g', alpha = 0.1)  # draws nodes
        nx.draw_networkx_edges(graph, pos, width=2.0, edge_color='r', alpha = 0.6)  # draws edges
        nx.draw_networkx_edge_labels(graph, pos, edge_labels=nx.get_edge_attributes(graph, 'label')) # edge lables
        nx.draw_networkx_labels(graph, pos, labels=nx.get_node_attributes(graph, 'label')) # node lables

        # nx.draw(graph, pos, node_size=50, with_labels=True)

        logger.info("%s", f">> save {down_node.section['section_name']}.png")
        plt.savefig(f'{down_node.section["section_name"]}.png') # save as png
        plt.box(False) # no frame around it
        plt.show() # display

        logger.info("%s", f">> write_network_text {down_node.section['section_name']}.nwk")
        # nx.write_network_text(graph, path=f'{section["section_name"]}.nwk', with_labels='label', max_depth=20, ascii_only=True, sources = [ getSectionKey(section) ])
        nx.write_network_text(graph, path=f'{down_node.section["section_name"]}.nwk', with_labels=True, ascii_only=True)

    def init_from_file_parsers(self, empty_soong_file_paths : Set[Path]):
        n : int = 0
        for file_path in empty_soong_file_paths:
            for project in self.projects.values():
                if project.path in file_path.parents:
                    n = n + 1
                    project.soong_files[file_path] = SoongFile(file_path)
                    logger.info("%d added a empty path %s into project %s", n, str(file_path), str(project.path))

        if n < len(empty_soong_file_paths):
            logger.error("missed some empty_soong_file_paths n:%d len:%d", n, len(empty_soong_file_paths))

        for path in empty_soong_file_paths:
            if not path.exists():
                logger.error("non-exist-soong-file-path %s", path)
                # raise LookupError

    def init_section_nodes(self, sections : Sequence[Section]) -> None:
        self.projects.clear()
        self.section_nodes.clear()

        self.init_all_module_types(sections)
        
        for section in sections:
            if has_invalid_target_arch(section):
                logger.info("ignore section with invalid target:%s section:%s", section['target_arch'], getSectionKey(section))
            else:
                self.section_nodes.add(SectionNode(section))

        for node in self.section_nodes:
            self.named_nodes_dict[node.section['section_name']].add(node)
            for name in get_backend_suffixed_names(node.section):
                self.named_nodes_dict[name].add(node)

        for node in self.section_nodes:
            project_path = node.section['project_path']
            soong_file_path = node.section['soong_file_path']
            if not isinstance(project_path, pathlib.Path) or not isinstance(soong_file_path, pathlib.Path):
                logger.info("project or soong is not a path")
            else:
                project = self.projects[project_path]
                if not isinstance(project, Project): # no project
                    project = Project(project_path)
                    self.projects[project_path] = project

                soong_file = project.soong_files[soong_file_path]
                if not isinstance(soong_file, SoongFile): # no soong file
                    soong_file = SoongFile(soong_file_path)
                    project.soong_files[soong_file_path] = soong_file

                if not node in soong_file.section_nodes: # no section
                    soong_file.section_nodes.add(node)

        for project_path, project in self.projects.items():
            logger.info("\t project:%s", project.path)
            for soong_file in project.soong_files.values():
                logger.info("\t\t soong file:%s", soong_file.path)
                for node in soong_file.section_nodes:
                    logger.info("\t\t\t section:%s/%s", node.section['section_name'], node.section['section_type'])

        self.expand_default_sections(self.section_nodes)

        logger.info("TOTALLY THERE ARE %d PROJECTS, %d SOONG FILES, %d SECTION NODES",
                    len(self.projects), sum(len(proj.soong_files.values()) for proj in self.projects.values()), len(self.section_nodes))

    def dir_provider(self) -> List[str]:
        """A choices provider is useful when the choice list is based on instance data of your application"""
        return self.query_dir_options

    # Parser for example command

    # # Tab complete from choices provided by a choices_provider
    # query_parser.add_argument(
    #     "direct",
    #     choices_provider=dir_provider,
    #     metavar="DIR",
    #     help="tab complete using a choices_provider"
    # )

    PROJECT : str = "project"
    SOONG_FILE : str = "soong_file"
    SECTION : str = "section"
    NODE : str = "node"
    ALL_MODULE_TYPES : str = "all_module_types"
    CHECK_CIRCULAR_DEP : str = "check_circular_dep"

    """Type of the section (e.g. cc_library)."""

    query_parser.add_argument('-u', '--up',
                              action='store_true',
                              help='list who depends on the specified scopes including project, Android.bp and target')
    query_parser.add_argument('-d', '--down',
                              action='store_true',
                              help='list targets on which the specified module depends')
    query_parser.add_argument('-a', '--all',
                              action='store_true',
                              help='list all dependencies including both the up and down')

    query_parser.add_argument(
        f"--{PROJECT}",
        choices_provider=project_provider,
        metavar="PROJECT",
        help="tab complete using a choices_provider"
    )

    query_parser.add_argument(
        f"--{SOONG_FILE}",
        choices_provider=soong_file_provider,
        metavar="SOONG",
        help="tab complete using a choices_provider"
    )

    query_parser.add_argument(
        f"--{SECTION}",
        choices_provider=section_provider,
        metavar="SECTION",
        help="specify the section with a name"
    )

    query_parser.add_argument('-l', '--max_levels', type=int, default=2, help='output [n] times')

    def find_soong_file(self, soong_path : Path) -> SoongFile:
        for project in self.projects.values():
            if soong_path in project.soong_files:
                return project.soong_files[soong_path]
        return None

    def find_section_node(self, project_path : str, soong_file_path : str, section_name : str) -> Set[SectionNode]:
        result : Set[SectionNode] = set()
        if soong_file_path:
            soong_file = self.find_soong_file(Path(soong_file_path))
            if isinstance(soong_file, SoongFile):
                for node in soong_file.section_nodes:
                    if (not section_name or node.section["section_name"] == section_name):
                        result.add(node)
        elif project_path:
            project = self.projects[project_path]
            if isinstance(project, Project):
                for sf in project.soong_files:
                    for node in sf.section_nodes:
                        if (not section_name or node.section["section_name"] == section_name):
                            result.add(node)
        else:
            for node in self.section_nodes:
                if (not section_name or node.section["section_name"] == section_name):
                    result.add(node)

        return result


    query_parser.add_argument('-V', '--verbose', action='store_true', help='for validate soong file command')
    query_parser.add_argument('-G', '--gen_graph', action='store_true', help='for validate soong file command')

    @with_argparser(query_parser)
    def do_validate_soong_file(self, args : argparse.Namespace):
        if args.soong_file:
            soong : SoongFile = self.find_soong_file(Path(args.soong_file))
            if not soong:
                logger.error("invalid soong file %s", args.soong_file)
            else:
                total_nodes = self.find_section_node(None, args.soong_file, None)
                logger.info("find %d of total nodes", len(total_nodes))
                if args.gen_graph:
                    for n in total_nodes:
                        edges : List[Tuple[SectionNode, SectionNode, Dict[any]]] = []
                        self.build_node_graph(self.UNLIMITED_LEVELS, n, edges)

                logger.info("dump all soong file upers %d", len(soong.up_soong_files))
                for i, sf in enumerate(soong.up_soong_files):
                    logger.info("%d up soong file:%s", i, sf.name())
                    for i, node in enumerate(sf.section_nodes):
                        logger.info("%s has node [%d]:%s", sf.name(), i, node.name2())
                        if args.verbose:
                            self.dump_both_levels_node(node, 0, args.max_levels, args.max_levels)


    @with_argparser(query_parser)
    def do_query(self, args: argparse.Namespace):
        """Repeats what you tell me to."""
        logger.info("got project:%s soong:%s section:%s", args.project, args.soong_file, args.section)
        nodes : Set[SectionNode] = self.find_section_node(args.project, args.soong_file, args.section)
        if not nodes:
            logger.error("find no section node!")
        else:
            for i, node in enumerate(nodes):
                logger.info(">> dump_both_levels_node [%d]:%s", i, node.name2())
                self.dump_both_levels_node(node, 0, args.max_levels, args.max_levels)

        # logger.info("gen_section_up_graph:[%s] max_levels:%d", node.name2(), args.max_levels)
        # for n in node.up_level_nodes:
        #     logger.info("---- up %s", n.name2())
        # for n in node.down_level_nodes:
        #     logger.info("---- down %s", n.name2())
        # self.gen_section_up_graph(node, args.max_levels)


    argparser = Cmd2ArgumentParser()
    argparser.add_argument('-p', '--piglatin', action='store_true', help='atinLay')
    argparser.add_argument('-s', '--shout', action='store_true', help='N00B EMULATION MODE')
    argparser.add_argument('-r', '--repeat', type=int, help='output [n] times')
    # argparser.add_argument('word', nargs='?', help='word to say')

    @with_argparser(argparser)
    def do_speak(self, opts):
        """Repeats what you tell me to."""
        arg = opts.word
        if opts.piglatin:
            arg = '%s%say' % (arg[1:], arg[0])
        if opts.shout:
            arg = arg.upper()
        repetitions = opts.repeat or 1
        for i in range(min(repetitions, 20)):
            self.poutput(arg)

    def set_pick_file(self, pickle_file : Path):
        self.pickle_file = pickle_file

    def do_exit(self, inp):
        print("Bye")
        return True

    def help_exit(self):
        print("exit the application. Shorthand: x q Ctrl-D.'")

    def do_add(self, inp):
        print(f"adding '{inp}'")

    def help_add(self):
        print("Add a new entry to the system.")

    def default(self, line):
        if line == 'x' or line == 'q':
            return self.do_exit(line)
 
        print("Default: {}".format(line))
 
    do_EOF = do_exit
    help_EOF = help_exit

app = typer.Typer()

@app.command(name="generate-single")
def generate_single(
    result_dir: Path = typer.Argument(
        ...,
        help="Where to store the result",
        dir_okay=True,
        exists=True,
        writable=True,
        resolve_path=True,
    ),
    custom_name: str = typer.Argument(
        ..., help="custom name for pickle file"
    ),
    aosp_root: str = typer.Argument(
        ...,
        help="Mirror directory for AOSP (either a link or a path)",
    ),
    workdir: Optional[Path] = typer.Option(
        None, help="Workdir", dir_okay=True, writable=True, exists=True
    ),
):
    """Generate a BGraph from a branch.

    It will work in the workdir and store results in result_dir.
    """

    aosp_root = pathlib.Path(aosp_root)
    logger.info(">> custom name:%s workdir:%s aosp-root:%s result_dir:%s", custom_name, workdir, aosp_root.absolute(), result_dir)

    # Assume the mirror is a Path if "http" is not found in mirror.
    workdir = bgraph.builder.compose_local_manifest_branch(pathlib.Path(aosp_root), custom_name, workdir)

    if workdir is None:
        typer.echo("Compose manifest failed.", err=True)
        raise typer.Exit(code=1)

    bgraph.builder.convert(workdir, result_dir)

gshell = GraphShell()

@app.command(name="load-pickle")
def load_pickle(
    file_path: Path = typer.Argument(
        ...,
        help="Where to load the pickle file",
        exists=True,
    ),
):
    """test load pickle file soong parser from a pickle file.
    """

    print("file_path is " + repr(file_path))

    with open(file_path, "rb") as file:
        soong_parser = pickle.load(file)

    print("sections len ", len(soong_parser.sections))
    print("section_nodes len ", len(soong_parser.section_nodes))

    gshell.init_section_nodes(soong_parser.sections.values())
    gshell.init_from_file_parsers(soong_parser.empty_soong_file_paths)

    sys.exit(gshell.cmdloop())

@app.command(name="backup-single")
def backup_single(
    module_path: Path = typer.Argument(
        ...,
        help="Where to store the result",
        dir_okay=True,
        exists=True,
        writable=True,
        resolve_path=True,
    ),
):
    """backup the specified module.
    """

    logger.info(">> backup single module path %s", str(module_path))
    path = backup_module(module_path, dry_run=False)
    logger.info("<< module path %s backuped to %s", str(module_path), str(path))


@app.command(name="restore-single")
def restore_single(
    module_path: Path = typer.Argument(
        ...,
        help="Where to store the result",
        dir_okay=True,
        exists=True,
        writable=True,
        resolve_path=True,
    ),
):
    """restore the specified module.
    """

    logger.info(">> restore single module path %s", str(module_path))
    path = restore_module(module_path, dry_run=False)
    logger.info("<< module path %s restoreed to %s", str(module_path), str(path))



@app.command()
def generate(
    result_dir: Path = typer.Argument(
        ...,
        dir_okay=True,
        exists=True,
        writable=True,
        resolve_path=True,
        help="Where to store the resulting BGraph",
    ),
    mirror: str = typer.Argument(
        ..., help="Path to the mirror or the URL to AOSP source"
    ),
    branch_pattern: str = typer.Option(
        "android-*", help="Pattern to match the branches"
    ),
    workdir: Optional[Path] = typer.Option(
        None, help="Work directory (default will be a tmp directory)"
    ),
):
    """Generate BGraph's from a mirror dir."""

    mirror_path: Union[str, pathlib.Path] = bgraph.utils.clean_mirror_path(mirror)

    workdir = bgraph.builder.compose_all(mirror_path, branch_pattern, workdir)

    bgraph.builder.convert(workdir, result_dir)
    founds = len(list(result_dir.glob("*.bgraph")))
    typer.echo(f"Generated {founds} graphs.")


@app.command(name="list")
def list_command(
    directory: Path = typer.Argument(
        None,
        file_okay=False,
        dir_okay=True,
        exists=True,
        readable=True,
        resolve_path=True,
        help="The directory to search BGraph files",
    ),
    extension: Optional[str] = typer.Option(
        ".bgraph", help="Extension of the BGraph files"
    ),
):
    """
    List the BGraph already generated.
    """

    table = rich.table.Table(title="BGraph founds :")
    table.add_column("Name", justify="right")
    table.add_column("Size", justify="right")

    for bgraph_file in directory.rglob(f"*{extension}"):

        table.add_row(
            bgraph_file.name, rich.filesize.decimal(bgraph_file.stat().st_size)
        )

    console = rich.console.Console()
    console.print(table)


@app.command()
def query(
    graph_path: Path = typer.Argument(
        ...,
        file_okay=True,
        dir_okay=False,
        exists=True,
        resolve_path=True,
        readable=True,
        help="BGraph to query",
    ),
    target: str = typer.Option(None, help="Target to query"),
    src: str = typer.Option(None, help="Source file"),
    dependency: str = typer.Option(None, "--dep", help="Dependecy"),
    out: OutChoice = typer.Option(OutChoice.TXT, help="Output format"),
):
    """Query a BGraph."""

    defined = [target is not None, src is not None, dependency is not None]
    if defined.count(True) > 1:
        typer.echo("Define only one of src/target/dependency")
        raise typer.Exit(code=1)

    try:
        graph = bgraph.viewer.load_graph(graph_path)
    except bgraph.exc.BGraphLoadingException:
        typer.echo("Unable to load the graph")
        raise typer.Exit(code=1)

    result: List[str]
    query_type: QueryType
    if target is not None:
        result = bgraph.viewer.find_sources(graph, target)
        query_type = QueryType.TARGET
        query_value = target
    elif src is not None:
        query_value, result = bgraph.viewer.find_target(graph, src)
        query_type = QueryType.SOURCE
    else:
        query_type = QueryType.DEPENDENCY
        result = bgraph.viewer.find_dependency(graph, dependency)
        query_value = dependency

    if not result:
        typer.echo("No result for request")
        raise typer.Exit(code=2)

    bgraph.viewer.format_result(graph, result, query_type, query_value, out)


@app.callback()
def main(
    logfile: Optional[str] = typer.Option(
        "gshell.log", help="work file path"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Activate verbose output"
    )
):
    """BGraph - generate and query build dependency graphes.

    BGraph is used to manipulate build dependency graphs generated from blueprint files.
    The main commands are:

        - generate : used to generates multiples graphs

        - query: used to query a previously generated graph

    To get more help, see the online documentation.
    """
    # logger.info("getrecursionlimit %d", sys.getrecursionlimit())
    # sys.setrecursionlimit(20000)

    gshell.debug = True
    logging_level = logging.INFO
    if verbose:
        logging_level = logging.DEBUG
    logging.getLogger().setLevel(logging_level)

    backup_file(f'logs/{logfile}')
    logging.basicConfig(filename=f'logs/{logfile}', encoding='utf-8', filemode='w', level=logging.DEBUG)
